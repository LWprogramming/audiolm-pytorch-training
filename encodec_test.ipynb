{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install encodec torchaudio torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNGLWXm-iMQM",
        "outputId": "af3ffd01-101f-49b8-b734-70cd4f7a93a2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting encodec\n",
            "  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (0.13.1+cu116)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from encodec) (1.22.4)\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Building wheels for collected packages: encodec\n",
            "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45775 sha256=a5ea932e765562c2c05d33eb974ddf223ed8fafddf99e45a8c8a4b3a5f81dcf8\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/9d/20/489d6aafffb505e18fcfcfbe722562f91c26af0a8a6da7d00b\n",
            "Successfully built encodec\n",
            "Installing collected packages: einops, encodec\n",
            "Successfully installed einops-0.6.0 encodec-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-OYS0aCiKA8",
        "outputId": "c63a166d-e472-4c2c-8ee5-20c90ea4d82c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "channels 2 and sampel rate 48000 and wav_shape torch.Size([1, 77040]) and sr 16000\n",
            "wav.shape torch.Size([1, 2, 231120])\n",
            "model.quantizer.bins 1024\n",
            "model.quantizer.dimension 128\n",
            "model.quantizer.n_q 16\n",
            "len encoded_frames 5\n",
            "encoded[0] shapes: [torch.Size([1, 4, 150]), torch.Size([1, 4, 150]), torch.Size([1, 4, 150]), torch.Size([1, 4, 150]), torch.Size([1, 4, 129])]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 729])"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "from encodec import EncodecModel\n",
        "from encodec.utils import convert_audio\n",
        "\n",
        "import torchaudio\n",
        "import torch\n",
        "\n",
        "# Instantiate a pretrained EnCodec model\n",
        "model = EncodecModel.encodec_model_48khz()\n",
        "model.set_target_bandwidth(12.0)\n",
        "\n",
        "# Load and pre-process the audio waveform\n",
        "wav, sr = torchaudio.load(\"test.wav\")\n",
        "print(f\"channels {model.channels} and sampel rate {model.sample_rate} and wav_shape {wav.shape} and sr {sr}\")\n",
        "# convert_audio up-samples if necessary, e.g. if wav has n samples at 16 kHz and model is 48 kHz, then resulting wav has 3n samples because you do n * 48/16\n",
        "wav = convert_audio(wav, sr, model.sample_rate, model.channels)\n",
        "# print(wav)\n",
        "# print(wav.shape)\n",
        "wav = wav.unsqueeze(0)\n",
        "print(f\"wav.shape {wav.shape}\")\n",
        "\n",
        "print(f\"model.quantizer.bins {model.quantizer.bins}\") # 1024 codebook size\n",
        "print(f\"model.quantizer.dimension {model.quantizer.dimension}\") # 128-dimension vecs in codebook\n",
        "print(f\"model.quantizer.n_q {model.quantizer.n_q}\") # 16 quantizers\n",
        "\n",
        "model.quantizer.get_num_quantizers_for_bandwidth(wav)\n",
        "# Extract discrete codes from EnCodec\n",
        "with torch.no_grad():\n",
        "    encoded_frames = model.encode(wav)\n",
        "    # Note that the 48 kHz model processes the audio by chunks of 1 seconds,\n",
        "    # with an overlap of 1%, and renormalizes the audio to have unit scale.\n",
        "    # For this model, the output of model.encode(wav) would a list\n",
        "    # (for each frame of 1 second) of a tuple (codes, scale) with scale a scalar tensor.\n",
        "\n",
        "print(f\"len encoded_frames {len(encoded_frames)}\")\n",
        "print(f\"encoded[0] shapes: {[encoded[0].shape for encoded in encoded_frames]}\")\n",
        "codes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1)  # [B, n_q, T]\n",
        "len(model.quantizer.vq.layers) # should be n_q\n",
        "# TODO: continue from here, why codes n_q not actually 16 then? what am I missing\n",
        "\n",
        "# # print(codes[0][0])\n",
        "\n",
        "codes.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = wav.shape[-1]\n",
        "num_seconds = num_samples / model.sample_rate\n",
        "num_frames = num_seconds * model.frame_rate\n",
        "num_frames / 0.99 # so i guess codes length is the number of segments\n",
        "# num_frames * model.segment_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qONw9u4n_gM",
        "outputId": "8c2e4f0c-c2aa-4253-f101-591cae45923e"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "729.5454545454546"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "??model.encode\n",
        " # 48000 == segment_length. for pretrained 48kHz model, model.segment is 1.0 seconds. so 48k samples\n",
        "model.segment_length\n",
        "# 47520 with overlap of 1%, i.e. segment_length * 0.99\n",
        "model.segment_stride\n",
        "\n",
        "# wav.shape has length 231120\n",
        "# 231120 / 47520 # 4.8636363...\n",
        "\n",
        "# the first batch x channel x 48000 samples\n",
        "x = wav[:, :, :model.segment_length]\n",
        "# length = x.shape[-1]\n",
        "# duration = length / model.sample_rate\n",
        "# duration\n",
        "encoded_frame = model._encode_frame(x)\n",
        "len(encoded_frame)\n",
        "encoded_frame[0].shape # batch x 4 x 150\n",
        "encoded_frame[1] # scale = 0.1256\n",
        "\n",
        "# inside _encode_frame\n",
        "model.normalize\n",
        "emb = model.encoder(x) # 1 x 128 x 150. downsampling ratio == 320, 150 * 320 == 48k\n",
        "# emb.shape\n",
        "model.frame_rate # frame_rate 150 == ceil(sampling rate / stride product). model.encoder.ratios are 2,4,5,8 -> 320\n",
        "model.bandwidth # 6.0 set by hand earlier\n",
        "# model.set_target_bandwidth(6.0) # bandwidth directly proportional to number of results that quantizer returns\n",
        "# bandwidth for 48kHz pretrained = 1.5 * the number of codes per timestep. no idea why\n",
        "\n",
        "codes = model.quantizer.encode(emb, model.frame_rate, model.bandwidth)\n",
        "codes.shape # 4 x 1 x 150\n",
        "# then the code just transposes to 1 x 4 x 150\n",
        "model.quantizer.get_num_quantizers_for_bandwidth(model.sample_rate, model.bandwidth)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSLI0IcqvbZJ",
        "outputId": "7d9765df-f3b7-439b-b442-0208a60dd565"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.quantizer.bins"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aksx1GIIEG-1",
        "outputId": "69f21e4b-f628-4b29-f31f-b9280f3a909c"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 24 kHz\n",
        "# torch.Size([1, 115560])\n",
        "# torch.Size([1, 1, 115560])\n",
        "# torch.Size([1, 8, 362])\n",
        "\n",
        "# 48 kHz\n",
        "# torch.Size([2, 231120])\n",
        "# torch.Size([1, 2, 231120]) # doubled along both fronts? oh wait the 2 is the two channels\n",
        "# torch.Size([1, 4, 729])\n",
        "\n",
        "codes.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NFmU5KOiYlt",
        "outputId": "497f67a9-1a88-4bb0-8bb3-1c1638e0e863"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 729])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model24 = EncodecModel.encodec_model_24khz()\n",
        "model.frame_rate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRNP2KVVjrCc",
        "outputId": "c17131be-7725-42b4-acf1-7b9b05bae244"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from encodec import EncodecModel\n",
        "from encodec.utils import convert_audio\n",
        "\n",
        "import torch\n",
        "\n",
        "# Instantiate a pretrained EnCodec model as described in README\n",
        "model = EncodecModel.encodec_model_24khz()\n",
        "model.set_target_bandwidth(6.0)\n",
        "x = torch.rand(1, 1, 24000) # batch x channel x samples. 1 second's worth\n",
        "with torch.no_grad():\n",
        "    encoded_frames = model.encode(x)\n",
        "codes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1)  # [B, n_q, T]\n",
        "codes.shape # 1 x 8 x 75, 75 being the frame-rate\n",
        "print(f\"model's n_q is {model.quantizer.n_q} but the result had {codes.shape[1]}\") # 32, 8\n",
        "\n",
        "# Zoom in on code in model.py _encode_frame method\n",
        "# https://github.com/facebookresearch/encodec/blob/f6a9f768373ba351d0cd18b928769df40da1aeb5/encodec/model.py#L147\n",
        "y = torch.rand(1, 1, 24000)\n",
        "emb = model.encoder(y) # 1 x 128 x 75. 75 is frames per second\n",
        "# then it calls model.quantizer.encode, so we jump into there next\n",
        "# codes = model.quantizer.encode(emb, model.frame_rate, model.bandwidth)\n",
        "# note the discrepancy here-- code passes model.frame_rate and not model.sample_rate\n",
        "# https://github.com/facebookresearch/encodec/blob/f6a9f768373ba351d0cd18b928769df40da1aeb5/encodec/quantization/vq.py#L100\n",
        "sample_rate_thats_secretly_frame_rate = model.frame_rate\n",
        "n_q = model.quantizer.get_num_quantizers_for_bandwidth(model.frame_rate, model.bandwidth)\n",
        "# frame_rate = 75, bandwidth = 6.\n",
        "# So get_bandwidth_per_quantizer is 0.75 in the incorrect version,\n",
        "# and n_q ends up being bandwidth / bandwidth_per_quantizer = 6 / 0.75 = 8\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAS5KLaYy8i4",
        "outputId": "512b5e66-950b-4878-f887-09743276267f"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 75])"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    }
  ]
}