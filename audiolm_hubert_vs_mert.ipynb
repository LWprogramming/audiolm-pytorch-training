{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "peitPTE5J-I9"
      },
      "outputs": [],
      "source": [
        "# pretrained substitute\n",
        "# Encodec as a replacement for SoundStream, and MERT as a replacement for w2v-BERT.\n",
        "# idea from https://github.com/zhvng/open-musiclm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch datasets\n",
        "\n",
        "# download audiolm_pytorch manually so i can inject print statements\n",
        "# !pip uninstall -y audiolm_pytorch\n",
        "import urllib.request\n",
        "import os\n",
        "import zipfile\n",
        "if not os.path.isfile(\"audiolm-pytorch.zip\"):\n",
        "  urllib.request.urlretrieve(\"https://github.com/lucidrains/audiolm-pytorch/archive/refs/heads/main.zip\", \"audiolm-pytorch.zip\")\n",
        "if not os.path.isdir(\"audiolm-pytorch\"):\n",
        "  with zipfile.ZipFile(\"audiolm-pytorch.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"audiolm-pytorch\")\n",
        "# !mv audiolm-pytorch/audiolm-pytorch-personal_hacks/audiolm_pytorch .\n",
        "!rm -rf audiolm-pytorch # not the one with underscore which is the actual library"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4q_lA_1KdJv",
        "outputId": "623a7608-dec2-4161-e557-b543c0289c75"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (3.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # semantic- MERT\n",
        "# # https://huggingface.co/m-a-p/MERT-v0\n",
        "# # MERT-v0 is a completely unsupervised model trained on 1000 hour music audios.\n",
        "\n",
        "# from transformers import Wav2Vec2Processor, HubertModel\n",
        "# import torch\n",
        "# from torch import nn\n",
        "# from datasets import load_dataset\n",
        "\n",
        "# # load demo audio and set processor\n",
        "# dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
        "# dataset = dataset.sort(\"id\")\n",
        "# sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
        "# processor = Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
        "\n",
        "# # loading our model weights\n",
        "# model = HubertModel.from_pretrained(\"m-a-p/MERT-v0\")\n",
        "\n",
        "# # audio file is decoded on the fly\n",
        "# inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
        "# with torch.no_grad():\n",
        "#     outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "# # take a look at the output shape, there are 13 layers of representation\n",
        "# # each layer performs differently in different downstream tasks, you should choose empirically\n",
        "# all_layer_hidden_states = torch.stack(outputs.hidden_states).squeeze()\n",
        "# print(all_layer_hidden_states.shape) # [13 layer, 292 timestep, 768 feature_dim]\n",
        "\n",
        "# # # for utterance level classification tasks, you can simply reduce the representation in time\n",
        "# # time_reduced_hidden_states = all_layer_hidden_states.mean(-2)\n",
        "# # print(time_reduced_hidden_states.shape) # [13, 768]\n",
        "\n",
        "# # # you can even use a learnable weighted average representation\n",
        "# # aggregator = nn.Conv1d(in_channels=13, out_channels=1, kernel_size=1)\n",
        "# # weighted_avg_hidden_states = aggregator(time_reduced_hidden_states.unsqueeze(0)).squeeze()\n",
        "# # print(weighted_avg_hidden_states.shape) # [768]\n"
      ],
      "metadata": {
        "id": "Vo5-Yc1qKMEB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# original semantic transformer\n",
        "import torch\n",
        "from audiolm_pytorch import HubertWithKmeans, SemanticTransformer, SemanticTransformerTrainer\n",
        "import os\n",
        "import urllib\n",
        "\n",
        "# hubert checkpoints can be downloaded at\n",
        "# https://github.com/facebookresearch/fairseq/tree/main/examples/hubert\n",
        "\n",
        "hubert_ckpt = 'hubert/hubert_base_ls960.pt'\n",
        "hubert_quantizer = f'hubert/hubert_base_ls960_L9_km500.bin' # listed in row \"HuBERT Base (~95M params)\", column Quantizer\n",
        "if not os.path.isdir(\"hubert\"):\n",
        "  os.makedirs(\"hubert\")\n",
        "if not os.path.isfile(hubert_ckpt):\n",
        "  hubert_ckpt_download = f\"https://dl.fbaipublicfiles.com/{hubert_ckpt}\"\n",
        "  urllib.request.urlretrieve(hubert_ckpt_download, f\"./{hubert_ckpt}\")\n",
        "if not os.path.isfile(hubert_quantizer):\n",
        "  hubert_quantizer_download = f\"https://dl.fbaipublicfiles.com/{hubert_quantizer}\"\n",
        "  urllib.request.urlretrieve(hubert_quantizer_download, f\"./{hubert_quantizer}\")\n",
        "\n",
        "wav2vec = HubertWithKmeans(\n",
        "    checkpoint_path = './hubert/hubert_base_ls960.pt',\n",
        "    kmeans_path = './hubert/hubert_base_ls960_L9_km500.bin'\n",
        ")\n",
        "\n",
        "semantic_transformer = SemanticTransformer(\n",
        "    num_semantic_tokens = wav2vec.codebook_size,\n",
        "    dim = 1024,\n",
        "    depth = 6\n",
        ").cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_EHyuEwLVVk",
        "outputId": "a3b53a5f-8b9c-440b-f5bc-b6e978668c0e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import wave\n",
        "# import struct\n",
        "\n",
        "# # dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
        "# # dataset[0][\"audio\"][\"array\"]\n",
        "# sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
        "\n",
        "# def save_wav(file_name, audio, sample_rate=sampling_rate):\n",
        "#   # Open up a wav file\n",
        "#   wav_file=wave.open(file_name,\"w\")\n",
        "#   # wav params\n",
        "#   nchannels = 1\n",
        "#   sampwidth = 2\n",
        "#   # 44100 is the industry standard sample rate - CD quality.  If you need to\n",
        "#   # save on file size you can adjust it downwards. The stanard for low quality\n",
        "#   # is 8000 or 8kHz.\n",
        "#   nframes = len(audio)\n",
        "#   comptype = \"NONE\"\n",
        "#   compname = \"not compressed\"\n",
        "#   wav_file.setparams((nchannels, sampwidth, sample_rate, nframes, comptype, compname))\n",
        "#   # WAV files here are using short, 16 bit, signed integers for the \n",
        "#   # sample size.  So we multiply the floating point data we have by 32767, the\n",
        "#   # maximum value for a short integer.  NOTE: It is theortically possible to\n",
        "#   # use the floating point -1.0 to 1.0 data directly in a WAV file but not\n",
        "#   # obvious how to do that using the wave module in python.\n",
        "#   for sample in audio:\n",
        "#     wav_file.writeframes(struct.pack('h', int( sample * 32767.0 )))\n",
        "#   wav_file.close()\n",
        "#   return\n",
        "# save_wav(\"test.wav\", dataset[1][\"audio\"][\"array\"])"
      ],
      "metadata": {
        "id": "jyQ8lrYiLwi7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from audiolm_pytorch import SemanticTransformerWrapper\n",
        "import numpy as np\n",
        "\n",
        "# in case not already loaded\n",
        "from datasets import load_dataset\n",
        "# load demo audio and set processor\n",
        "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
        "\n",
        "batch_size = 2\n",
        "# sample data is 77040 samples at 16kHz sampling rate\n",
        "# just reshape it here so batch size for prime_wave is effectively 1\n",
        "samples = np.array([dataset[1][\"audio\"][\"array\"], dataset[1][\"audio\"][\"array\"]])\n",
        "prime_wave = torch.tensor(samples).reshape(2, 77040).cuda()\n",
        "# raise AssertionError(prime_wave.shape)\n",
        "max_length = 2048\n",
        "semantic = SemanticTransformerWrapper(\n",
        "            wav2vec = wav2vec,\n",
        "            transformer = semantic_transformer,\n",
        "            audio_conditioner = None,\n",
        "            unique_consecutive = True\n",
        "        ).cuda()\n",
        "semantic_tokens = semantic.generate(\n",
        "            text_embeds = None, # no text, it's not musicLM\n",
        "            batch_size = batch_size,\n",
        "            prime_wave = prime_wave,\n",
        "            max_length = max_length\n",
        "        )\n",
        "# semantic.device # should be cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        },
        "id": "756nwPfnNLw1",
        "outputId": "3e637f6d-1395-422d-8e71-1ae0f4115083"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset librispeech_asr_demo (/root/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_demo/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embed.keys(): dict_keys(['x', 'padding_mask', 'features'])\n",
            "embed['x'] shape: torch.Size([2, 240, 768]), embed['features'].shape: torch.Size([2, 240, 768])\n",
            "wav_input shape: torch.Size([2, 77040]), embed shape: torch.Size([480, 768]), packed_shape: [torch.Size([2, 240])]\n",
            "codebook_indices before unpacking: torch.Size([480])\n",
            "codebook_indices after unpacking: torch.Size([2, 240])\n",
            "ids.shape: torch.Size([2, 240]) and prime_wave True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "generating semantic:  17%|█▋        | 324/1905 [00:10<00:52, 30.37it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-11be373fc09e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0munique_consecutive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         ).cuda()\n\u001b[0;32m---> 22\u001b[0;31m semantic_tokens = semantic.generate(\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mtext_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# no text, it's not musicLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/audiolm_pytorch/audiolm_pytorch.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mwas_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwas_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<@beartype(audiolm_pytorch.audiolm_pytorch.SemanticTransformerWrapper.generate) at 0x7fcf8e4a9280>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(__beartype_func, __beartype_conf, __beartype_get_violation, __beartype_object_9474080, __beartype_getrandbits, *args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/content/audiolm_pytorch/audiolm_pytorch.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, max_length, text, text_embeds, prime_wave, prime_ids, batch_size, cond_scale, filter_thres, temperature, include_eos_in_output, **kwargs)\u001b[0m\n\u001b[1;32m   1043\u001b[0m             \u001b[0msample_semantic_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_semantic_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mall_rows_have_eos_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_semantic_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ?semantic.wav2vec\n",
        "# torch.tensor(dataset[1][\"audio\"][\"array\"]).cuda().device\n",
        "# semantic_tokens.shape\n",
        "# semantic_tokens[:,-1] # unfortunately doesn't seem to be the EOS we're looking for\n",
        "# semantic_tokens[:, 0]"
      ],
      "metadata": {
        "id": "Ov9QvzEfXbhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset[1][\"audio\"][\"array\"].shape\n",
        "# # len(dataset[1][\"audio\"][\"array\"]) # 77040\n",
        "# # dataset.features[\"audio\"].sampling_rate # 16000\n",
        "# # so 4.815 seconds of audio"
      ],
      "metadata": {
        "id": "9eJXEn_lYpsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.empty((batch_size, 0), dtype = torch.long)"
      ],
      "metadata": {
        "id": "CzzhgEW-pekh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "biq3EnSui7dd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}